{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6Og7msupuaUi"
      ],
      "authorship_tag": "ABX9TyNlHsHFk1Eg7i+uUFyvmgYs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JihunSKKU/PySpark/blob/main/PySpark_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Practice - Lec07"
      ],
      "metadata": {
        "id": "6Og7msupuaUi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3O0FL4zrx6d",
        "outputId": "adfd9533-f210-43e0-e0e7-a006d42ba73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark RDD: Data conversion, Data save"
      ],
      "metadata": {
        "id": "TOuLYOBkuOy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "str_data = [\"RDD data 1\", \"RDD data 2\"]\n",
        "rdd_data = sc.parallelize(str_data)\n",
        "print(rdd_data)"
      ],
      "metadata": {
        "id": "ufC8T3ikr53V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3478f85-fc5e-44ed-d1e9-a9d165d6c3e8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[69] at readRDDFromFile at PythonRDD.scala:289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_in_hdfs = sc.textFile(\"/content/sorting_input/sorting.txt\")\n",
        "print(data_in_hdfs.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhZ7GBq8tUIG",
        "outputId": "8b74b59c-4d18-413e-c541-db5aaa20d9bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['82, 58, 49, 99, 25, 37, 44, 94, 70, 18, 73, 81, 32, 21, 79, 13, 38, 51, 70, 38, 82, 43, 42', '63, 78, 39, 1, 56, 35, 93, 66, 5, 91, 77, 86, 45, 58, 77, 49, 47, 15, 92, 12', '70, 38, 49, 58, 96, 79, 69, 94, 50, 11, 86, 39, 58, 74, 69, 82, 97', '84, 92, 92, 80, 98, 44, 27, 55, 52, 47, 44, 96, 64, 37, 30, 18, 18, 49, 53, 43', '84, 68, 38, 78, 23, 16, 41, 55, 50, 89, 9, 78, 23, 95, 24, 29, 21, 4, 71']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark는 Lazy execution이기 때문에 액션(Action)을 할 때 까지 변환(Transformation)이 실행되지 않고, 이로 인해 error가 발생하지 않음"
      ],
      "metadata": {
        "id": "CgeNQkavvEFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no_data_in_hdfs = sc.textFile(\"/content/sorting_input/no_sorting.txt\")\n",
        "# print(no_data_in_hdfs.collect())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FcHukJsctYZN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_in_hdfs = sc.textFile(\"/content/sorting_input/sorting.txt\")\n",
        "\n",
        "# Save data in new path\n",
        "data_in_hdfs.saveAsTextFile(\"/content/tmp/sorting_input\")\n",
        "\n",
        "# Inspect saved data\n",
        "data_in_hdfs_read = sc.textFile(\"/content/tmp/sorting_input/*\")\n",
        "print(data_in_hdfs_read.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztfB919MtekW",
        "outputId": "b34c8ae0-be5d-49c7-e984-ae0d94625601"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['82, 58, 49, 99, 25, 37, 44, 94, 70, 18, 73, 81, 32, 21, 79, 13, 38, 51, 70, 38, 82, 43, 42', '63, 78, 39, 1, 56, 35, 93, 66, 5, 91, 77, 86, 45, 58, 77, 49, 47, 15, 92, 12', '70, 38, 49, 58, 96, 79, 69, 94, 50, 11, 86, 39, 58, 74, 69, 82, 97', '84, 92, 92, 80, 98, 44, 27, 55, 52, 47, 44, 96, 64, 37, 30, 18, 18, 49, 53, 43', '84, 68, 38, 78, 23, 16, 41, 55, 50, 89, 9, 78, 23, 95, 24, 29, 21, 4, 71']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark RDD: Transformation"
      ],
      "metadata": {
        "id": "3BXUpPZKuQd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마찬가지로 Spark는 Lazy execution이기 때문"
      ],
      "metadata": {
        "id": "aQ8avaB6uyTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([\"A\", \"B\", \"C\", \"D\"])\n",
        "DD_squared = RDD.map(lambda x : x * x)"
      ],
      "metadata": {
        "id": "MDdm_Kd1t1UQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RDD = sc.parallelize([\"A\", \"B\", \"C\", \"D\"])\n",
        "# RDD_squared = RDD.map(lambda x : x * x).collect()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zG3L_1TwuibO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([1, 2, 3, 4, 5])\n",
        "RDD_squared1 = RDD.map(lambda x : x * x).collect()\n",
        "print(RDD_squared1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd5J5xdGujdX",
        "outputId": "63911c00-c8c3-4ddf-c2a1-bc153378f802"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "collect가 없으면 연산 값이 아닌 RDD 값이 반환됨"
      ],
      "metadata": {
        "id": "LBI_VuFJvSmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([1, 2, 3, 4, 5])\n",
        "RDD_squared1 = RDD.map(lambda x : x * x)\n",
        "print(RDD_squared1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRoGQ5mMuwTG",
        "outputId": "ee2e5a3e-e488-4ff0-f6a9-94cc6f7c492a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[83] at RDD at PythonRDD.scala:53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "def f(x):\n",
        "    return x*x\n",
        "\n",
        "RDD_squared2 = RDD.map(f).collect()\n",
        "print(RDD_squared2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyBPiRDXvQy3",
        "outputId": "66a9fabf-bd08-4beb-a1ac-8bd881253bc8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([\"spark,example\", \"flatmap,map\"])\n",
        "RDD_from_flatMap = RDD.map(lambda x : x.split(\",\")).collect()\n",
        "print(RDD_from_flatMap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOcg7RDjvtR2",
        "outputId": "ec99032b-40fc-461e-9390-81479333f92d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['spark', 'example'], ['flatmap', 'map']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([\"spark,example\", \"flatmap,map\"])\n",
        "RDD_from_flatMap = RDD.flatMap(lambda x : x.split(\",\")).collect()\n",
        "print(RDD_from_flatMap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Zw9T1vvf2C",
        "outputId": "d708d95c-132f-435a-86f8-fd7e02c808c5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['spark', 'example', 'flatmap', 'map']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data = sc.parallelize([123, 123, 456, 456, 456, 789])\n",
        "rdd_data_from_distinct = rdd_data.distinct()\n",
        "data_from_collect = rdd_data_from_distinct.collect()\n",
        "\n",
        "print(rdd_data_from_distinct)\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(data_from_collect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz67tRd9vi_R",
        "outputId": "e63820bb-dff7-4058-f05e-ec0d4a6014f4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[95] at collect at <ipython-input-29-6f80831578f5>:3\n",
            "-------------------------------------------------------\n",
            "[456, 123, 789]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([1, 2, 3, 4, 5])\n",
        "RDD_from_filter = RDD.filter(lambda x : x < 3).collect()\n",
        "print(RDD_from_filter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUhodOlIv0zq",
        "outputId": "84aef09c-6f21-4b74-ce5c-f84bbf3702e4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD1 = sc.parallelize([1, 1, 2, 2, 3, 3, 4, 4])\n",
        "RDD2 = sc.parallelize([3, 4, 4, 5])\n",
        "RDD3 = sc.parallelize([5, 6, 6])\n",
        "\n",
        "data_from_union = sc.union([RDD1, RDD2, RDD3]).collect()\n",
        "data_from_intersection = RDD1.intersection(RDD2).collect()\n",
        "data_from_subtract = RDD1.subtract(RDD2).collect()\n",
        "\n",
        "print(data_from_union)\n",
        "print(data_from_intersection)\n",
        "print(data_from_subtract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlZo4KK0v7bl",
        "outputId": "baef521e-b0e6-4535-c415-4eee45bda6a4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2, 2, 3, 3, 4, 4, 3, 4, 4, 5, 5, 6, 6]\n",
            "[4, 3]\n",
            "[1, 1, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise"
      ],
      "metadata": {
        "id": "vC7urNeY7H2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1"
      ],
      "metadata": {
        "id": "016cqNlb7NRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([ [[\"A\"],[\"B\"]], [[\"A\"],[\"B\"]] ])\n",
        "RDD_from_flatMap = RDD.flatMap(lambda x : x+x).collect()\n",
        "print(RDD_from_flatMap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pjCFNFo7M6l",
        "outputId": "6f1c3e6d-be86-4e40-fdb1-9bf46334cc0b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['A'], ['B'], ['A'], ['B'], ['A'], ['B'], ['A'], ['B']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2"
      ],
      "metadata": {
        "id": "CTogUQ2d7dR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD1 = sc.parallelize([\"ABC\", \"ABC\", \"ABC\", \"DEF\", \"DEF\"])\n",
        "RDD2 = sc.parallelize([\"ABC\", \"ABC\"])\n",
        "RDD3 = sc.parallelize([\"ABC\", \"ABC\"])\n",
        "\n",
        "data_from_union = sc.union([RDD1, RDD2, RDD3]).collect()\n",
        "print(data_from_union)\n",
        "\n",
        "data_from_intersection = RDD1.intersection(RDD2).collect()\n",
        "print(data_from_intersection)\n",
        "\n",
        "data_from_subtract = RDD2.subtract(RDD3).collect()\n",
        "print(data_from_subtract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPLFljVC7L-K",
        "outputId": "e060f280-6588-4fbe-bd6a-e70885d1cde7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ABC', 'ABC', 'ABC', 'DEF', 'DEF', 'ABC', 'ABC', 'ABC', 'ABC']\n",
            "['ABC']\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3"
      ],
      "metadata": {
        "id": "SBojvTpz7qrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "def f(x):\n",
        "    return x==1\n",
        "RDD_from_filter = RDD.filter(f).collect()\n",
        "\n",
        "print(RDD_from_filter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thhlOqKP7EpC",
        "outputId": "1a89863f-8701-49a4-d1d8-5e14d741d765"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([1, 2, 3, \"a\", \"c\", \"b\"])\n",
        "\n",
        "def f(x):\n",
        "    return type(x) == int\n",
        "RDD_from_filter = RDD.filter(f).collect()\n",
        "\n",
        "print(RDD_from_filter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR16zAqz7yL1",
        "outputId": "e1fff6a0-0cd5-4dce-9cfc-f6fb3096fbd1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark RDD: Action"
      ],
      "metadata": {
        "id": "Otbt8UHz75Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add # addition function\n",
        "RDD = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "RDD_from_reduce1 = RDD.reduce(add) # add all values in RDD\n",
        "\n",
        "def f(x, y):\n",
        "    return x + y\n",
        "\n",
        "RDD_from_reduce2 = RDD.reduce(f)\n",
        "\n",
        "print(RDD_from_reduce1)\n",
        "print(RDD_from_reduce2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10NCvXWH73CP",
        "outputId": "0f7faa73-70eb-4b0e-9dc3-c0016b0b4ecc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize(['a', 'c', 'b', 'e', 's'])\n",
        "def f(x, y):\n",
        "    return x + y\n",
        "RDD_from_reduce = RDD.reduce(f)\n",
        "print(RDD_from_reduce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7Mqp8yh8in7",
        "outputId": "de7c4316-d4fc-460d-b23f-de4e172ab452"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acbes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data = sc.parallelize([123, 123, 456, 456, 456, 789])\n",
        "number_of_data = rdd_data.count()\n",
        "print(number_of_data)\n",
        "\n",
        "rdd_data = sc.parallelize(['ABC','ABC','ABC','ABC','ABC','ABC'])\n",
        "number_of_data = rdd_data.count()\n",
        "print(number_of_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8vo9SoW8nof",
        "outputId": "b4134225-9fdd-47a7-e390-6e31e167011c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "data_from_collect = rdd_data.collect()\n",
        "data_from_first = rdd_data.first()\n",
        "print(data_from_collect)\n",
        "print(data_from_first)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmdFgs1j8pwh",
        "outputId": "765919b1-7f3e-4c34-fc93-2af72c8b5ed9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "data_from_take = rdd_data.take(3)\n",
        "print(data_from_take)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlfXNOwZ8uIZ",
        "outputId": "eddd7b14-8642-4375-bdea-bbd16c2db005"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "takeSample Argument\n",
        "    First:  Sampling with replacement or not\n",
        "    Second: Number of samples\n",
        "    Third:  Random seed\n",
        "'''\n",
        "data_from_takeSample = rdd_data.takeSample(False, 2, 2021)\n",
        "print(data_from_takeSample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zSBN4Fg8v1R",
        "outputId": "0e27bc8a-7c0c-458d-cfd1-17a693e747f6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([5, 3, 2, 4, 1])\n",
        "\n",
        "data_top2 = RDD.top(2)\n",
        "data_min = RDD.min()\n",
        "data_max = RDD.max()\n",
        "\n",
        "print(data_top2)\n",
        "print(data_min)\n",
        "print(data_max)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI8cQyS08yOv",
        "outputId": "d7841f2a-bddb-424a-da5e-32dcf20eb8f3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 4]\n",
            "1\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([\"A\", \"C\", \"D\", \"A\", \"B\"])\n",
        "\n",
        "data_top2 = RDD.top(2)\n",
        "data_min = RDD.min()\n",
        "data_max = RDD.max()\n",
        "\n",
        "print(data_top2)\n",
        "print(data_min)\n",
        "print(data_max)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIt0AvIA9RQS",
        "outputId": "a78769bc-f38f-4b4d-90bd-b4ade3c4bfa1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['D', 'C']\n",
            "A\n",
            "D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# No operations are performed on non-numeric values\n",
        "RDD = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "data_sum = RDD.sum()\n",
        "data_mean = RDD.mean()\n",
        "data_variance = RDD.variance()\n",
        "\n",
        "print(data_sum)\n",
        "print(data_mean)\n",
        "print(data_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWL63_3p9VHN",
        "outputId": "64086374-e95f-4b34-f1db-42520a9c8e44"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "3.0\n",
            "2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4"
      ],
      "metadata": {
        "id": "5_5nZNb89kTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "data_from_takeSample1 = rdd_data.takeSample(False, 2, 2021)\n",
        "data_from_takeSample2 = rdd_data.takeSample(False, 2, 2021)\n",
        "data_from_takeSample3 = rdd_data.takeSample(False, 2, 2021)\n",
        "data_from_takeSample4 = rdd_data.takeSample(False, 2, 0)\n",
        "data_from_takeSample5 = rdd_data.takeSample(False, 2, 0)\n",
        "data_from_takeSample6 = rdd_data.takeSample(False, 2, 1)\n",
        "\n",
        "print(data_from_takeSample1)\n",
        "print(data_from_takeSample2)\n",
        "print(data_from_takeSample3)\n",
        "print(data_from_takeSample4)\n",
        "print(data_from_takeSample5)\n",
        "print(data_from_takeSample6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YloU5aya9cDD",
        "outputId": "1ecefdbd-03b8-4abf-8ac2-aed923bf8227"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 8]\n",
            "[3, 8]\n",
            "[3, 8]\n",
            "[8, 9]\n",
            "[8, 9]\n",
            "[7, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Pair RDDs"
      ],
      "metadata": {
        "id": "bei1Iix79rjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([(\"spark\", [1, 2, 3]), (\"example\", [1, 2])])\n",
        "data_from_keys = RDD.keys().collect()\n",
        "data_from_values = RDD.values().collect()\n",
        "\n",
        "print(data_from_keys)\n",
        "print(data_from_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyPitIg69oSt",
        "outputId": "4b5dc999-c6f3-4973-d0c3-159e1ef7ba9c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['spark', 'example']\n",
            "[[1, 2, 3], [1, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD1 = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 2)])\n",
        "RDD2 = sc.parallelize([(\"a\", 2), (\"b\", 3), (\"b\", 4)])\n",
        "\n",
        "data_from_join = RDD1.join(RDD2).collect()\n",
        "print(data_from_join)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia7xBniU93cl",
        "outputId": "ee882093-898a-4eb9-d013-5f9a6e026ee8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('b', (2, 3)), ('b', (2, 4)), ('a', (1, 2))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([(\"spark\", [1, 2]), (\"spark\", [1, 4]), (\"example\", [1, 2])])\n",
        "def f(x):\n",
        "    return sum(x)\n",
        "data_from_mapValues = RDD.mapValues(f).collect()\n",
        "\n",
        "print(data_from_mapValues)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0dfNYZe964Q",
        "outputId": "6b089c13-1a62-43c2-fe2d-8b5331cc4314"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('spark', 3), ('spark', 5), ('example', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2), (\"a\", 2)])\n",
        "data_from_groupByKey_list = RDD.groupByKey().mapValues(list).collect()\n",
        "\n",
        "print(data_from_groupByKey_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMFT5F_Q-D8y",
        "outputId": "04b6ea00-7506-42d5-c93f-4bf5fccd4ebe"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('b', [1]), ('a', [1, 2, 2])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2), (\"a\", 2)])\n",
        "data_from_groupByKey_list = RDD.groupByKey().mapValues(sum).collect()\n",
        "\n",
        "print(data_from_groupByKey_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGgc7f82-V_7",
        "outputId": "2d4e25bf-fb87-4b83-ab02-9cd99c37e731"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('b', 1), ('a', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "\n",
        "RDD = sc.parallelize([(\"spark\", 1), (\"spark\", 1), (\"example\", 1)])\n",
        "data_from_reduceByKey = RDD.reduceByKey(add).collect()\n",
        "\n",
        "print(data_from_reduceByKey)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVbno8AJ-XWU",
        "outputId": "05b6efc7-1d26-4425-a992-909b108e3bfa"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('spark', 2), ('example', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize ([(\"a\", 1), (\"b\", 1), (\"a\", 2), (\"a\", 2)])\n",
        "\n",
        "def createCombiner(a): return [a] # merge results into a list\n",
        "\n",
        "def mergeValue(a, b):\n",
        "    a.append(b)\n",
        "    return a\n",
        "\n",
        "def mergeCombiners(a, b):\n",
        "    a.extend(b)\n",
        "    return a\n",
        "\n",
        "RDD.combineByKey(createCombiner, mergeValue, mergeCombiners).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i9L-LBk-jCe",
        "outputId": "f934bbfa-beb2-41e5-b182-666c8fcde342"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', [1]), ('a', [1, 2, 2])]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize ([(\"a\", 0), (\"a\", 2)])\n",
        "def createCombiner(a): return a\n",
        "def mergeValue(a, b):\n",
        "    return a*b\n",
        "def mergeCombiners(a, b):\n",
        "    return a+b\n",
        "\n",
        "RDD.combineByKey(createCombiner, mergeValue, mergeCombiners).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btZFySDQ-z8g",
        "outputId": "266d642e-2111-4fb8-b6da-028c8a7fb86c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# '<' not supported between instances of 'str' and 'int'\n",
        "RDD = sc.parallelize([(\"a\", [1, 2, 3]), (\"c\", [1]), (\"a\", [1, 2])])\n",
        "data_from_sortByKey = RDD.sortByKey().collect()\n",
        "\n",
        "print(data_from_sortByKey)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM3qVuXL-94G",
        "outputId": "9bb80afc-a37e-4b6d-8cf2-467020b2f114"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', [1, 2, 3]), ('a', [1, 2]), ('c', [1])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5"
      ],
      "metadata": {
        "id": "f5Bvrwvc_HoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "\n",
        "RDD = sc.parallelize([(\"spark\", \"a\"), (\"spark\", \"b\"), (\"example\", \"c\")])\n",
        "data_from_reduceByKey = RDD.reduceByKey(add).collect()\n",
        "\n",
        "print(data_from_reduceByKey)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMP5ABd0_Aom",
        "outputId": "d75d83e7-e5f9-41bc-a497-4ffd922d730d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('spark', 'ab'), ('example', 'c')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([(\"a\", [1, 2]), (\"b\", [1, 2, 3]), (\"c\", [1])])\n",
        "data_from_lookup = RDD.lookup(\"b\")\n",
        "print(data_from_lookup)\n",
        "\n",
        "RDD = sc.parallelize([(\"a\", [1]), (\"b\", [1, 2, 3]), (\"b\", [1, 2, 3])])\n",
        "data_from_lookup = RDD.lookup(\"b\")\n",
        "print(data_from_lookup)\n",
        "\n",
        "RDD = sc.parallelize([(\"a\", [1, 2]), (\"b\", [1, 2, 3]), (\"c\", [1])])\n",
        "data_from_lookup = RDD.lookup(\"d\")\n",
        "print(data_from_lookup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqepdk03_Jrg",
        "outputId": "b1461801-1231-40bc-f13d-59c30eadd9c9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3]]\n",
            "[[1, 2, 3], [1, 2, 3]]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([(\"a\", [1, 2]), (\"b\", [1, 2, 3]), (\"c\", [1])])\n",
        "data_from_collectAsMap = RDD.collectAsMap()\n",
        "print(data_from_collectAsMap)\n",
        "\n",
        "RDD = sc.parallelize([(\"a\", 1), (\"b\", 3), (\"c\", [1])])\n",
        "data_from_collectAsMap = RDD.collectAsMap()\n",
        "print(data_from_collectAsMap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1cbDRz0_TFY",
        "outputId": "fbfd37d3-5a03-45f1-8544-5582918043e8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': [1, 2], 'b': [1, 2, 3], 'c': [1]}\n",
            "{'a': 1, 'b': 3, 'c': [1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = sc.parallelize([(\"a\", [1, 2, 3]), (\"c\", [1]), (\"a\", [1, 2])])\n",
        "data_from_countByKey = RDD.countByKey()\n",
        "print(data_from_countByKey)\n",
        "\n",
        "RDD = sc.parallelize([(1, [1, 2, 3]), (1, 2), (\"a\", [1]), (3, [1, 2]), (\"b\", [1])])\n",
        "data_from_countByKey = RDD.countByKey()\n",
        "print(data_from_countByKey)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsae81B7_W0b",
        "outputId": "ef6b2245-fc57-406b-93af-1ec504c97bc6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'a': 2, 'c': 1})\n",
            "defaultdict(<class 'int'>, {1: 2, 'a': 1, 3: 1, 'b': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data = sc.parallelize([123, 123, 456, 456, 456, 789])\n",
        "data_from_countByValue = rdd_data.countByValue()\n",
        "print(data_from_countByValue)\n",
        "\n",
        "rdd_data = sc.parallelize([\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"])\n",
        "data_from_countByValue = rdd_data.countByValue()\n",
        "print(data_from_countByValue)"
      ],
      "metadata": {
        "id": "87gSkpNV_bnr",
        "outputId": "6dea02e3-8549-4498-ce2b-ee3c818bb607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {123: 2, 456: 3, 789: 1})\n",
            "defaultdict(<class 'int'>, {'A': 3, 'B': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSQL, SparkML Practice - Lec08"
      ],
      "metadata": {
        "id": "zSdR_mD9k7qG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving a DataFrame, Convert to a DataFrame"
      ],
      "metadata": {
        "id": "HSGj2RUnm9y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "\n",
        "sc = SparkContext()\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "id": "Ll0CdZz3k7Bq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert input data to DataFrame\n",
        "**Row()**\n",
        "- Makes possible to access like the attributes in RDBMS\n",
        "- e.g., row1 = Row(age=11, name=‘Alice’), \\\n",
        "    row1.name ⇒ ‘Alice’, row1.age ⇒ 11\n",
        "\n",
        "**toDF()**\n",
        "- Convert RDD to DataFrame"
      ],
      "metadata": {
        "id": "S0JzITt3nEbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Suwon'}),\n",
        "Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Seoul'})])\n",
        "\n",
        "DF = RDD.toDF() # RDD -> DataFrame\n",
        "DF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXa5iCJ8lIBs",
        "outputId": "722c002f-47b4-46d2-ae4d-6ad22922e9d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------+\n",
            "|dept_id|dept_info                  |\n",
            "+-------+---------------------------+\n",
            "|1      |{name -> CS, loc -> Seoul} |\n",
            "|2      |{name -> CS, loc -> Suwon} |\n",
            "|3      |{name -> R&D, loc -> Seoul}|\n",
            "+-------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save DataFrame in JSON format"
      ],
      "metadata": {
        "id": "Izq12bDJnqY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "DF = sc.parallelize([\n",
        "Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Suwon'}),\n",
        "Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Seoul'})]).toDF()\n",
        "\n",
        "DF.write.json(\"./SKKU-DBP-24/DF_json\")"
      ],
      "metadata": {
        "id": "R7Ed60QxlVqS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/SKKU-DBP-24/DF_json"
      ],
      "metadata": {
        "id": "9RRaA6N-la-P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert json data to DataFrame"
      ],
      "metadata": {
        "id": "pQrxPt22nwUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sqlCtx = SQLContext(sc)\n",
        "\n",
        "RDD = sc.textFile(\"./SKKU-DBP-24/DF_json/*\").map(lambda x: json.loads(x))\n",
        "\n",
        "DF = sqlCtx.createDataFrame(RDD)\n",
        "\n",
        "DF.show(truncate=False) # truncate=False: show all data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebIU7-07mH-u",
        "outputId": "41573757-a6d5-4d3b-a681-00672a53abf5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------+\n",
            "|dept_id|dept_info                  |\n",
            "+-------+---------------------------+\n",
            "|1      |{name -> CS, loc -> Seoul} |\n",
            "|2      |{name -> CS, loc -> Suwon} |\n",
            "|3      |{name -> R&D, loc -> Seoul}|\n",
            "+-------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sqlCtx = SQLContext(sc)\n",
        "DF = sqlCtx.read.json(\"./SKKU-DBP-24/DF_json/*\")\n",
        "DF.registerTempTable(\"dept\")\n",
        "\n",
        "DF.show(truncate=False) # truncate=False: show all data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAtzxarymIO_",
        "outputId": "1a2dd9ea-7f8d-4e10-e79c-0ea3618685e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+\n",
            "|dept_id|dept_info   |\n",
            "+-------+------------+\n",
            "|2      |{Suwon, CS} |\n",
            "|3      |{Seoul, R&D}|\n",
            "|1      |{Seoul, CS} |\n",
            "+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark SQL Operations"
      ],
      "metadata": {
        "id": "dto4SqeGn01B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})\n",
        "])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "DF.select(\"dept_info.name\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrffboi2mc4P",
        "outputId": "0e1bb710-9f2f-459c-dea1-c071c979bfcf"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|CS  |\n",
            "|CS  |\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})\n",
        "])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "DF.select(\"dept_id\",\"dept_info.loc\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymWkcVU6n7xS",
        "outputId": "2ba9d05a-1316-479b-d5f8-bd41fb5df828"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|dept_id|loc  |\n",
            "+-------+-----+\n",
            "|1      |Seoul|\n",
            "|2      |Suwon|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### select(column_name): 3ways"
      ],
      "metadata": {
        "id": "QcOBMXjLrT-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})\n",
        "])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "## First\n",
        "DF.select(DF.dept_id).show()\n",
        "\n",
        "## second\n",
        "from pyspark.sql.functions import col\n",
        "DF.select(col(\"dept_id\")).show()\n",
        "\n",
        "## Third\n",
        "DF.select(\"dept_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIcnS7k-n_KL",
        "outputId": "8ec2f887-1291-43a2-9ed4-80ff4d3ca270"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|dept_id|\n",
            "+-------+\n",
            "|      1|\n",
            "|      2|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|dept_id|\n",
            "+-------+\n",
            "|      1|\n",
            "|      2|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|dept_id|\n",
            "+-------+\n",
            "|      1|\n",
            "|      2|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert type of column"
      ],
      "metadata": {
        "id": "2G9lHFwLrXsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})\n",
        "])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "DF_id = DF.withColumn(\"dept_id\", DF[\"dept_id\"].cast(IntegerType()))\n",
        "DF_id.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4UmFsuBoCiX",
        "outputId": "25249f5c-8fb1-4235-c22b-9cb89496957e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_id: integer (nullable = true)\n",
            " |-- dept_info: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add new column"
      ],
      "metadata": {
        "id": "NsC3GQo5rsi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "from pyspark.sql.functions import lit # Create a column of the literal value\n",
        "DF_with_new1 = DF.withColumn(\"new1\", lit(0))\n",
        "DF_with_new1.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import exp\n",
        "DF_with_new2 = DF.withColumn(\"new2\", exp(\"dept_id\"))\n",
        "DF_with_new2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs425qaqrXBy",
        "outputId": "99bd7faf-5a5c-4a8b-c6aa-48e8ad285755"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------------+----+\n",
            "|dept_id|dept_info                 |new1|\n",
            "+-------+--------------------------+----+\n",
            "|1      |{name -> CS, loc -> Seoul}|0   |\n",
            "|2      |{name -> CS, loc -> Suwon}|0   |\n",
            "+-------+--------------------------+----+\n",
            "\n",
            "+-------+--------------------------+------------------+\n",
            "|dept_id|dept_info                 |new2              |\n",
            "+-------+--------------------------+------------------+\n",
            "|1      |{name -> CS, loc -> Seoul}|2.7182818284590455|\n",
            "|2      |{name -> CS, loc -> Suwon}|7.38905609893065  |\n",
            "+-------+--------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add new row"
      ],
      "metadata": {
        "id": "9mVmbGJmrwGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# DF1 with 2 rows\n",
        "DF1 = sc.parallelize([\n",
        "Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'})]).toDF()\n",
        "\n",
        "# DF2 with 1 row\n",
        "DF2 = sc.parallelize([\n",
        "Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "DF1.unionByName(DF2).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Klv6121mrlay",
        "outputId": "644534c3-963e-4e0a-e2f1-7600c7a1595d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------+\n",
            "|dept_id|dept_info                  |\n",
            "+-------+---------------------------+\n",
            "|1      |{name -> CS, loc -> Seoul} |\n",
            "|2      |{name -> CS, loc -> Busan} |\n",
            "|3      |{name -> R&D, loc -> Suwon}|\n",
            "+-------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove existing Column"
      ],
      "metadata": {
        "id": "g5mEHhVPrzA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "\n",
        "DF.drop(\"dept_id\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtbbV-Jor0uY",
        "outputId": "3fbc3626-97a2-4828-a4dc-01bb450b05cf"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+\n",
            "|dept_info                  |\n",
            "+---------------------------+\n",
            "|{name -> CS, loc -> Seoul} |\n",
            "|{name -> CS, loc -> Busan} |\n",
            "|{name -> R&D, loc -> Suwon}|\n",
            "+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract row"
      ],
      "metadata": {
        "id": "kyJo8gFjr5Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "\n",
        "# Avoid sub table\n",
        "DF_flatten = DF.select('dept_id', 'dept_info.name', 'dept_info.loc')\n",
        "\n",
        "# Extract row that meets the condition\n",
        "DF_flatten.filter(DF_flatten[\"name\"] == \"CS\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzKvzb-2r3VJ",
        "outputId": "a610c6c1-d2d3-4c81-d9a4-7177455af186"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+\n",
            "|dept_id|name|  loc|\n",
            "+-------+----+-----+\n",
            "|      1|  CS|Seoul|\n",
            "|      2|  CS|Busan|\n",
            "+-------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### others function"
      ],
      "metadata": {
        "id": "A4Bp6LnrsEht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "\n",
        "# Avoid sub-table\n",
        "DF_flatten = DF.select(\"dept_id\", \"dept_info.name\", \"dept_info.loc\")\n",
        "DF_flatten.groupBy(\"name\").count().show() # get number of data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLT85JbZr9Cb",
        "outputId": "3fe5a9a9-27d7-4e70-daa9-856608e282fe"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|name|count|\n",
            "+----+-----+\n",
            "|  CS|    2|\n",
            "| R&D|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF1 = sc.parallelize([Row(dept_id='1', name='CS', loc= 'Suwon'),\n",
        "    Row(dept_id='2', name='CS', loc= 'Busan')]).toDF()\n",
        "\n",
        "DF2 = sc.parallelize([Row(dept_id='3', name='R&D', loc= 'Seoul'),\n",
        "    Row(dept_id='4', name='R&D', loc='Busan')]).toDF()\n",
        "\n",
        "DF_join = DF1.join(DF2, DF1[\"loc\"] == DF2[\"loc\"]) # join\n",
        "DF_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vhPOzfnsEJO",
        "outputId": "72acfc84-dc5e-4d7e-f856-05a76c76f3c2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+-------+----+-----+\n",
            "|dept_id|name|  loc|dept_id|name|  loc|\n",
            "+-------+----+-----+-------+----+-----+\n",
            "|      2|  CS|Busan|      4| R&D|Busan|\n",
            "+-------+----+-----+-------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize(\n",
        "    [Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "\n",
        "# Avoid sub-table\n",
        "DF_flatten = DF.select('dept_id', 'dept_info.name', 'dept_info.loc')\n",
        "\n",
        "# Convert DataFrame to RDD\n",
        "RDD_from_DF = DF_flatten.rdd.map(lambda x: x.name)\n",
        "print(RDD_from_DF.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFVMC8qTsNel",
        "outputId": "69eef218-9862-4430-98a0-984bd812308c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CS', 'CS', 'R&D']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Seoul'})]).toDF()\n",
        "\n",
        "DF.createTempView(\"temp_table\")\n",
        "\n",
        "sqlDF = spark.sql(\"SELECT dept_info.name \\\n",
        "    FROM temp_table \\\n",
        "    WHERE dept_info.loc==\\\"Seoul\\\"\")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-VS5Vm4sS98",
        "outputId": "da6ed3bd-9d59-4358-f601-4ac5d5d81ef2"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|  CS|\n",
            "| R&D|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.dropTempView(\"temp_table\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooUWjnnnsYzx",
        "outputId": "03edc4ad-c80b-447d-e76c-de38e1054508"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Seoul'})]).toDF()\n",
        "\n",
        "DF.createOrReplaceTempView(\"temp_table\")\n",
        "\n",
        "sqlDF = spark.sql(\"SELECT dept_info.name \\\n",
        "    FROM temp_table \\\n",
        "    WHERE dept_info.loc==\\\"Seoul\\\"\")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaQKFL7qsgBk",
        "outputId": "50b0e14b-ed08-42e6-ec5a-486e30dcdba9"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|  CS|\n",
            "| R&D|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark ML library - Clustering\n",
        "\n",
        "K-means clustering algorithm\n",
        "1. Initialize centroids\n",
        "2. Assign cluster to each data according to its nearest centroid\n",
        "3. Update centroids towards the center of data\n",
        "4. Repeat 2, 3 until centroids remain unchanged"
      ],
      "metadata": {
        "id": "-rJtHiVYsm6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "spark = SparkSession.builder.appName(\"KMeansPractice\").getOrCreate()\n",
        "\n",
        "data = [[0.0, 0.1], [1.0, 1.0], [0.5, 0.6], [0.5, 2.0], [9.0, 8.0], [8.0, 9.0], [9.0, 9.5], [10.0, 10.0]]\n",
        "columns = [\"feature1\", \"feature2\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kdmOpqHsmQ8",
        "outputId": "c6015862-3e11-42a8-ba5c-fdd9b528e69c"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+\n",
            "|feature1|feature2|\n",
            "+--------+--------+\n",
            "|     0.0|     0.1|\n",
            "|     1.0|     1.0|\n",
            "|     0.5|     0.6|\n",
            "|     0.5|     2.0|\n",
            "|     9.0|     8.0|\n",
            "|     8.0|     9.0|\n",
            "|     9.0|     9.5|\n",
            "|    10.0|    10.0|\n",
            "+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
        "\n",
        "df = assembler.transform(df)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_oLaG7ts9ea",
        "outputId": "9999a4a5-4152-4fd2-e94f-eb3284d0b88a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+-----------+\n",
            "|feature1|feature2|   features|\n",
            "+--------+--------+-----------+\n",
            "|     0.0|     0.1|  [0.0,0.1]|\n",
            "|     1.0|     1.0|  [1.0,1.0]|\n",
            "|     0.5|     0.6|  [0.5,0.6]|\n",
            "|     0.5|     2.0|  [0.5,2.0]|\n",
            "|     9.0|     8.0|  [9.0,8.0]|\n",
            "|     8.0|     9.0|  [8.0,9.0]|\n",
            "|     9.0|     9.5|  [9.0,9.5]|\n",
            "|    10.0|    10.0|[10.0,10.0]|\n",
            "+--------+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the K-means model\n",
        "- KMeans(featuresCol, predictionCol, k, maxIter, distanceMeasure)\n",
        "    - featuresCol: Features column name.\n",
        "    - predictionCol: Prediction column name.\n",
        "    - k: The number of clusters to create.\n",
        "    - maxIter: Max number of iteration.\n",
        "    - distanceMeasure: The distance measure. (euclidean/cosine)\n"
      ],
      "metadata": {
        "id": "dy8M8IZ6tHeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(featuresCol='features',\n",
        "                predictionCol='prediction',\n",
        "                k=2,\n",
        "                maxIter=20,\n",
        "                distanceMeasure='euclidean')\n",
        "\n",
        "model = kmeans.fit(df)"
      ],
      "metadata": {
        "id": "f1tnRBsEtE9B"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centroids = model.clusterCenters()\n",
        "predictions = model.transform(df).select(\"features\", \"prediction\")"
      ],
      "metadata": {
        "id": "EjZZ83yttaDn"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cluster centroids:\")\n",
        "for cent in centroids:\n",
        "    print(cent)\n",
        "\n",
        "print(\"Result:\")\n",
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NznFxm-Ptb8B",
        "outputId": "38cc160a-a535-4c3f-a364-64022998ec1d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster centroids:\n",
            "[9.    9.125]\n",
            "[0.5   0.925]\n",
            "Result:\n",
            "+-----------+----------+\n",
            "|   features|prediction|\n",
            "+-----------+----------+\n",
            "|  [0.0,0.1]|         1|\n",
            "|  [1.0,1.0]|         1|\n",
            "|  [0.5,0.6]|         1|\n",
            "|  [0.5,2.0]|         1|\n",
            "|  [9.0,8.0]|         0|\n",
            "|  [8.0,9.0]|         0|\n",
            "|  [9.0,9.5]|         0|\n",
            "|[10.0,10.0]|         0|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark ML library - Classification"
      ],
      "metadata": {
        "id": "nzBiKysFstlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "F2KiGl37wXz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "spark = SparkSession.builder.appName(\"IrisLogisticRegression\").getOrCreate()\n",
        "\n",
        "iris_data = spark.read.option('header', 'true').option('inferSchema', 'true').csv('Iris.csv')"
      ],
      "metadata": {
        "id": "_ulVMtU8s9D7"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "idx_dict = {'Iris-setosa': 0,\n",
        "            'Iris-versicolor': 1,\n",
        "            'Iris-virginica': 2}\n",
        "\n",
        "# User Define Function\n",
        "label_mapping_udf = udf(lambda label: idx_dict.get(label), IntegerType())\n",
        "iris_data = iris_data.withColumn(\"Species\", label_mapping_udf(iris_data[\"Species\"]))\n",
        "iris_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twa3ULuksuzP",
        "outputId": "3c2e36bf-0520-482d-e233-201d47af10d9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------------+-------------+------------+-------+\n",
            "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|\n",
            "+---+-------------+------------+-------------+------------+-------+\n",
            "|  1|          5.1|         3.5|          1.4|         0.2|      0|\n",
            "|  2|          4.9|         3.0|          1.4|         0.2|      0|\n",
            "|  3|          4.7|         3.2|          1.3|         0.2|      0|\n",
            "|  4|          4.6|         3.1|          1.5|         0.2|      0|\n",
            "|  5|          5.0|         3.6|          1.4|         0.2|      0|\n",
            "|  6|          5.4|         3.9|          1.7|         0.4|      0|\n",
            "|  7|          4.6|         3.4|          1.4|         0.3|      0|\n",
            "|  8|          5.0|         3.4|          1.5|         0.2|      0|\n",
            "|  9|          4.4|         2.9|          1.4|         0.2|      0|\n",
            "| 10|          4.9|         3.1|          1.5|         0.1|      0|\n",
            "| 11|          5.4|         3.7|          1.5|         0.2|      0|\n",
            "| 12|          4.8|         3.4|          1.6|         0.2|      0|\n",
            "| 13|          4.8|         3.0|          1.4|         0.1|      0|\n",
            "| 14|          4.3|         3.0|          1.1|         0.1|      0|\n",
            "| 15|          5.8|         4.0|          1.2|         0.2|      0|\n",
            "| 16|          5.7|         4.4|          1.5|         0.4|      0|\n",
            "| 17|          5.4|         3.9|          1.3|         0.4|      0|\n",
            "| 18|          5.1|         3.5|          1.4|         0.3|      0|\n",
            "| 19|          5.7|         3.8|          1.7|         0.3|      0|\n",
            "| 20|          5.1|         3.8|          1.5|         0.3|      0|\n",
            "+---+-------------+------------+-------------+------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the features into a vector column and name the column to \"features\"\n",
        "assembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n",
        "                            outputCol=\"features\")\n",
        "\n",
        "# Rename the target column to \"label\"\n",
        "iris_data = assembler.transform(iris_data).select(\"features\", \"Species\").withColumnRenamed(\"Species\", \"label\")"
      ],
      "metadata": {
        "id": "b5laqOWdufZZ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = iris_data.randomSplit([0.8, 0.2], seed=2023)"
      ],
      "metadata": {
        "id": "sjyqb42OuusD"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "logistic_regression = LogisticRegression(featuresCol=\"features\",\n",
        "                                         labelCol=\"label\",\n",
        "                                         predictionCol='prediction',\n",
        "                                         maxIter=100)\n",
        "\n",
        "model = logistic_regression.fit(train_data)"
      ],
      "metadata": {
        "id": "oC_lhCLwuwjU"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5LpEI8uu2BX",
        "outputId": "53d30a6d-1421-410c-99eb-4b8247246044"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel: uid=LogisticRegression_06b914faa6d6, numClasses=3, numFeatures=4"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(test_data)"
      ],
      "metadata": {
        "id": "sw9okWngvcwx"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1i_7piRwKaZ",
        "outputId": "b026795e-0927-404c-d1f7-9c676a7a9ce1"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+--------------------+--------------------+----------+\n",
            "|         features|label|       rawPrediction|         probability|prediction|\n",
            "+-----------------+-----+--------------------+--------------------+----------+\n",
            "|[4.7,3.2,1.3,0.2]|    0|[9438.32169347803...|       [1.0,0.0,0.0]|       0.0|\n",
            "|[4.9,3.1,1.5,0.1]|    0|[8794.92750919444...|       [1.0,0.0,0.0]|       0.0|\n",
            "|[5.0,3.6,1.4,0.2]|    0|[9604.88573039206...|       [1.0,0.0,0.0]|       0.0|\n",
            "|[5.4,3.9,1.7,0.4]|    0|[8318.84741194077...|       [1.0,0.0,0.0]|       0.0|\n",
            "|[5.5,2.4,3.7,1.0]|    1|[-711.8096549038,...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.5,2.5,4.0,1.3]|    1|[-1965.4704489795...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.6,3.0,4.1,1.3]|    1|[-1128.9673545014...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.6,3.0,4.5,1.5]|    1|[-2440.3856093591...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.7,2.8,4.1,1.3]|    1|[-1826.3269796827...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.8,2.7,4.1,1.0]|    1|[-1258.7122062005...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.1,2.8,4.7,1.2]|    1|[-3285.6376804798...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.1,3.0,4.9,1.8]|    2|[-5163.7950144830...|[0.0,2.4006923202...|       2.0|\n",
            "|[6.2,2.2,4.5,1.5]|    1|[-5658.1697700489...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,2.3,4.4,1.3]|    1|[-4791.2041381852...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,2.5,4.9,1.5]|    1|[-5776.7266613810...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,2.8,5.1,1.5]|    2|[-5366.5285956567...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,3.3,4.7,1.6]|    1|[-3870.5243538655...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,3.4,5.6,2.4]|    2|[-7773.5665309372...|       [0.0,0.0,1.0]|       2.0|\n",
            "|[6.4,2.7,5.3,1.9]|    2|[-7501.3849517314...|       [0.0,0.0,1.0]|       2.0|\n",
            "|[6.4,2.8,5.6,2.2]|    2|[-8755.0457458071...|       [0.0,0.0,1.0]|       2.0|\n",
            "+-----------------+-----+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7kdWC0awM0F",
        "outputId": "56c0cca6-4687-497d-e596-d3df3bc1d7b0"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fypJDEgEwRrF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
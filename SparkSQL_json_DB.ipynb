{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7PH6p+zXvChwopGXutRHj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JihunSKKU/PySpark/blob/main/SparkSQL_json_DB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSQL - Lec08"
      ],
      "metadata": {
        "id": "zSdR_mD9k7qG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving a DataFrame, Convert to a DataFrame"
      ],
      "metadata": {
        "id": "HSGj2RUnm9y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "\n",
        "sc = SparkContext()\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "id": "Ll0CdZz3k7Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert input data to DataFrame\n",
        "**Row()**\n",
        "- Makes possible to access like the attributes in RDBMS\n",
        "- e.g., row1 = Row(age=11, name=‘Alice’), \\\n",
        "    row1.name ⇒ ‘Alice’, row1.age ⇒ 11\n",
        "\n",
        "**toDF()**\n",
        "- Convert RDD to DataFrame"
      ],
      "metadata": {
        "id": "S0JzITt3nEbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Suwon'}),\n",
        "Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Seoul'})])\n",
        "\n",
        "DF = RDD.toDF() # RDD -> DataFrame\n",
        "DF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXa5iCJ8lIBs",
        "outputId": "722c002f-47b4-46d2-ae4d-6ad22922e9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------+\n",
            "|dept_id|dept_info                  |\n",
            "+-------+---------------------------+\n",
            "|1      |{name -> CS, loc -> Seoul} |\n",
            "|2      |{name -> CS, loc -> Suwon} |\n",
            "|3      |{name -> R&D, loc -> Seoul}|\n",
            "+-------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save DataFrame in JSON format"
      ],
      "metadata": {
        "id": "Izq12bDJnqY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "DF = sc.parallelize([\n",
        "Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Suwon'}),\n",
        "Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Seoul'})]).toDF()\n",
        "\n",
        "DF.write.json(\"./SKKU-DBP-24/DF_json\")"
      ],
      "metadata": {
        "id": "R7Ed60QxlVqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/SKKU-DBP-24/DF_json"
      ],
      "metadata": {
        "id": "9RRaA6N-la-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert json data to DataFrame"
      ],
      "metadata": {
        "id": "pQrxPt22nwUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sqlCtx = SQLContext(sc)\n",
        "\n",
        "RDD = sc.textFile(\"./SKKU-DBP-24/DF_json/*\").map(lambda x: json.loads(x))\n",
        "\n",
        "DF = sqlCtx.createDataFrame(RDD)\n",
        "\n",
        "DF.show(truncate=False) # truncate=False: show all data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebIU7-07mH-u",
        "outputId": "41573757-a6d5-4d3b-a681-00672a53abf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------+\n",
            "|dept_id|dept_info                  |\n",
            "+-------+---------------------------+\n",
            "|1      |{name -> CS, loc -> Seoul} |\n",
            "|2      |{name -> CS, loc -> Suwon} |\n",
            "|3      |{name -> R&D, loc -> Seoul}|\n",
            "+-------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sqlCtx = SQLContext(sc)\n",
        "DF = sqlCtx.read.json(\"./SKKU-DBP-24/DF_json/*\")\n",
        "DF.registerTempTable(\"dept\")\n",
        "\n",
        "DF.show(truncate=False) # truncate=False: show all data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAtzxarymIO_",
        "outputId": "1a2dd9ea-7f8d-4e10-e79c-0ea3618685e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+\n",
            "|dept_id|dept_info   |\n",
            "+-------+------------+\n",
            "|2      |{Suwon, CS} |\n",
            "|3      |{Seoul, R&D}|\n",
            "|1      |{Seoul, CS} |\n",
            "+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark SQL Operations"
      ],
      "metadata": {
        "id": "dto4SqeGn01B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})\n",
        "])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "DF.select(\"dept_info.name\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrffboi2mc4P",
        "outputId": "0e1bb710-9f2f-459c-dea1-c071c979bfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|CS  |\n",
            "|CS  |\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})\n",
        "])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "DF.select(\"dept_id\",\"dept_info.loc\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymWkcVU6n7xS",
        "outputId": "2ba9d05a-1316-479b-d5f8-bd41fb5df828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|dept_id|loc  |\n",
            "+-------+-----+\n",
            "|1      |Seoul|\n",
            "|2      |Suwon|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### select(column_name): 3ways"
      ],
      "metadata": {
        "id": "QcOBMXjLrT-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})\n",
        "])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "## First\n",
        "DF.select(DF.dept_id).show()\n",
        "\n",
        "## second\n",
        "from pyspark.sql.functions import col\n",
        "DF.select(col(\"dept_id\")).show()\n",
        "\n",
        "## Third\n",
        "DF.select(\"dept_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIcnS7k-n_KL",
        "outputId": "8ec2f887-1291-43a2-9ed4-80ff4d3ca270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|dept_id|\n",
            "+-------+\n",
            "|      1|\n",
            "|      2|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|dept_id|\n",
            "+-------+\n",
            "|      1|\n",
            "|      2|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|dept_id|\n",
            "+-------+\n",
            "|      1|\n",
            "|      2|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert type of column"
      ],
      "metadata": {
        "id": "2G9lHFwLrXsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})\n",
        "])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "DF_id = DF.withColumn(\"dept_id\", DF[\"dept_id\"].cast(IntegerType()))\n",
        "DF_id.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4UmFsuBoCiX",
        "outputId": "25249f5c-8fb1-4235-c22b-9cb89496957e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_id: integer (nullable = true)\n",
            " |-- dept_info: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add new column"
      ],
      "metadata": {
        "id": "NsC3GQo5rsi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "RDD = sc.parallelize([\n",
        "    Row(dept_id='1',dept_info={'name':'CS', 'loc':'Seoul'}),\n",
        "    Row(dept_id='2',dept_info={'name':'CS', 'loc':'Suwon'})])\n",
        "DF = RDD.toDF()\n",
        "\n",
        "from pyspark.sql.functions import lit # Create a column of the literal value\n",
        "DF_with_new1 = DF.withColumn(\"new1\", lit(0))\n",
        "DF_with_new1.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import exp\n",
        "DF_with_new2 = DF.withColumn(\"new2\", exp(\"dept_id\"))\n",
        "DF_with_new2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs425qaqrXBy",
        "outputId": "99bd7faf-5a5c-4a8b-c6aa-48e8ad285755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------------+----+\n",
            "|dept_id|dept_info                 |new1|\n",
            "+-------+--------------------------+----+\n",
            "|1      |{name -> CS, loc -> Seoul}|0   |\n",
            "|2      |{name -> CS, loc -> Suwon}|0   |\n",
            "+-------+--------------------------+----+\n",
            "\n",
            "+-------+--------------------------+------------------+\n",
            "|dept_id|dept_info                 |new2              |\n",
            "+-------+--------------------------+------------------+\n",
            "|1      |{name -> CS, loc -> Seoul}|2.7182818284590455|\n",
            "|2      |{name -> CS, loc -> Suwon}|7.38905609893065  |\n",
            "+-------+--------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add new row"
      ],
      "metadata": {
        "id": "9mVmbGJmrwGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# DF1 with 2 rows\n",
        "DF1 = sc.parallelize([\n",
        "Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'})]).toDF()\n",
        "\n",
        "# DF2 with 1 row\n",
        "DF2 = sc.parallelize([\n",
        "Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "DF1.unionByName(DF2).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Klv6121mrlay",
        "outputId": "644534c3-963e-4e0a-e2f1-7600c7a1595d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------+\n",
            "|dept_id|dept_info                  |\n",
            "+-------+---------------------------+\n",
            "|1      |{name -> CS, loc -> Seoul} |\n",
            "|2      |{name -> CS, loc -> Busan} |\n",
            "|3      |{name -> R&D, loc -> Suwon}|\n",
            "+-------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove existing Column"
      ],
      "metadata": {
        "id": "g5mEHhVPrzA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "\n",
        "DF.drop(\"dept_id\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtbbV-Jor0uY",
        "outputId": "3fbc3626-97a2-4828-a4dc-01bb450b05cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+\n",
            "|dept_info                  |\n",
            "+---------------------------+\n",
            "|{name -> CS, loc -> Seoul} |\n",
            "|{name -> CS, loc -> Busan} |\n",
            "|{name -> R&D, loc -> Suwon}|\n",
            "+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract row"
      ],
      "metadata": {
        "id": "kyJo8gFjr5Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "\n",
        "# Avoid sub table\n",
        "DF_flatten = DF.select('dept_id', 'dept_info.name', 'dept_info.loc')\n",
        "\n",
        "# Extract row that meets the condition\n",
        "DF_flatten.filter(DF_flatten[\"name\"] == \"CS\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzKvzb-2r3VJ",
        "outputId": "a610c6c1-d2d3-4c81-d9a4-7177455af186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+\n",
            "|dept_id|name|  loc|\n",
            "+-------+----+-----+\n",
            "|      1|  CS|Seoul|\n",
            "|      2|  CS|Busan|\n",
            "+-------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### others function"
      ],
      "metadata": {
        "id": "A4Bp6LnrsEht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "\n",
        "# Avoid sub-table\n",
        "DF_flatten = DF.select(\"dept_id\", \"dept_info.name\", \"dept_info.loc\")\n",
        "DF_flatten.groupBy(\"name\").count().show() # get number of data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLT85JbZr9Cb",
        "outputId": "3fe5a9a9-27d7-4e70-daa9-856608e282fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|name|count|\n",
            "+----+-----+\n",
            "|  CS|    2|\n",
            "| R&D|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF1 = sc.parallelize([Row(dept_id='1', name='CS', loc= 'Suwon'),\n",
        "    Row(dept_id='2', name='CS', loc= 'Busan')]).toDF()\n",
        "\n",
        "DF2 = sc.parallelize([Row(dept_id='3', name='R&D', loc= 'Seoul'),\n",
        "    Row(dept_id='4', name='R&D', loc='Busan')]).toDF()\n",
        "\n",
        "DF_join = DF1.join(DF2, DF1[\"loc\"] == DF2[\"loc\"]) # join\n",
        "DF_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vhPOzfnsEJO",
        "outputId": "72acfc84-dc5e-4d7e-f856-05a76c76f3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+-------+----+-----+\n",
            "|dept_id|name|  loc|dept_id|name|  loc|\n",
            "+-------+----+-----+-------+----+-----+\n",
            "|      2|  CS|Busan|      4| R&D|Busan|\n",
            "+-------+----+-----+-------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize(\n",
        "    [Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Suwon'})]).toDF()\n",
        "\n",
        "# Avoid sub-table\n",
        "DF_flatten = DF.select('dept_id', 'dept_info.name', 'dept_info.loc')\n",
        "\n",
        "# Convert DataFrame to RDD\n",
        "RDD_from_DF = DF_flatten.rdd.map(lambda x: x.name)\n",
        "print(RDD_from_DF.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFVMC8qTsNel",
        "outputId": "69eef218-9862-4430-98a0-984bd812308c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CS', 'CS', 'R&D']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Seoul'})]).toDF()\n",
        "\n",
        "DF.createTempView(\"temp_table\")\n",
        "\n",
        "sqlDF = spark.sql(\"SELECT dept_info.name \\\n",
        "    FROM temp_table \\\n",
        "    WHERE dept_info.loc==\\\"Seoul\\\"\")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-VS5Vm4sS98",
        "outputId": "da6ed3bd-9d59-4358-f601-4ac5d5d81ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|  CS|\n",
            "| R&D|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.dropTempView(\"temp_table\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooUWjnnnsYzx",
        "outputId": "03edc4ad-c80b-447d-e76c-de38e1054508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "DF = sc.parallelize([\n",
        "    Row(dept_id='1', dept_info={'name': 'CS', 'loc': 'Seoul'}),\n",
        "    Row(dept_id='2', dept_info={'name': 'CS', 'loc': 'Busan'}),\n",
        "    Row(dept_id='3', dept_info={'name': 'R&D', 'loc': 'Seoul'})]).toDF()\n",
        "\n",
        "DF.createOrReplaceTempView(\"temp_table\")\n",
        "\n",
        "sqlDF = spark.sql(\"SELECT dept_info.name \\\n",
        "    FROM temp_table \\\n",
        "    WHERE dept_info.loc==\\\"Seoul\\\"\")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaQKFL7qsgBk",
        "outputId": "50b0e14b-ed08-42e6-ec5a-486e30dcdba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|  CS|\n",
            "| R&D|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark ML library - Clustering\n",
        "\n",
        "K-means clustering algorithm\n",
        "1. Initialize centroids\n",
        "2. Assign cluster to each data according to its nearest centroid\n",
        "3. Update centroids towards the center of data\n",
        "4. Repeat 2, 3 until centroids remain unchanged"
      ],
      "metadata": {
        "id": "-rJtHiVYsm6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "spark = SparkSession.builder.appName(\"KMeansPractice\").getOrCreate()\n",
        "\n",
        "data = [[0.0, 0.1], [1.0, 1.0], [0.5, 0.6], [0.5, 2.0], [9.0, 8.0], [8.0, 9.0], [9.0, 9.5], [10.0, 10.0]]\n",
        "columns = [\"feature1\", \"feature2\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kdmOpqHsmQ8",
        "outputId": "c6015862-3e11-42a8-ba5c-fdd9b528e69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+\n",
            "|feature1|feature2|\n",
            "+--------+--------+\n",
            "|     0.0|     0.1|\n",
            "|     1.0|     1.0|\n",
            "|     0.5|     0.6|\n",
            "|     0.5|     2.0|\n",
            "|     9.0|     8.0|\n",
            "|     8.0|     9.0|\n",
            "|     9.0|     9.5|\n",
            "|    10.0|    10.0|\n",
            "+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
        "\n",
        "df = assembler.transform(df)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_oLaG7ts9ea",
        "outputId": "9999a4a5-4152-4fd2-e94f-eb3284d0b88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+-----------+\n",
            "|feature1|feature2|   features|\n",
            "+--------+--------+-----------+\n",
            "|     0.0|     0.1|  [0.0,0.1]|\n",
            "|     1.0|     1.0|  [1.0,1.0]|\n",
            "|     0.5|     0.6|  [0.5,0.6]|\n",
            "|     0.5|     2.0|  [0.5,2.0]|\n",
            "|     9.0|     8.0|  [9.0,8.0]|\n",
            "|     8.0|     9.0|  [8.0,9.0]|\n",
            "|     9.0|     9.5|  [9.0,9.5]|\n",
            "|    10.0|    10.0|[10.0,10.0]|\n",
            "+--------+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the K-means model\n",
        "- KMeans(featuresCol, predictionCol, k, maxIter, distanceMeasure)\n",
        "    - featuresCol: Features column name.\n",
        "    - predictionCol: Prediction column name.\n",
        "    - k: The number of clusters to create.\n",
        "    - maxIter: Max number of iteration.\n",
        "    - distanceMeasure: The distance measure. (euclidean/cosine)\n"
      ],
      "metadata": {
        "id": "dy8M8IZ6tHeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(featuresCol='features',\n",
        "                predictionCol='prediction',\n",
        "                k=2,\n",
        "                maxIter=20,\n",
        "                distanceMeasure='euclidean')\n",
        "\n",
        "model = kmeans.fit(df)"
      ],
      "metadata": {
        "id": "f1tnRBsEtE9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centroids = model.clusterCenters()\n",
        "predictions = model.transform(df).select(\"features\", \"prediction\")"
      ],
      "metadata": {
        "id": "EjZZ83yttaDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cluster centroids:\")\n",
        "for cent in centroids:\n",
        "    print(cent)\n",
        "\n",
        "print(\"Result:\")\n",
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NznFxm-Ptb8B",
        "outputId": "38cc160a-a535-4c3f-a364-64022998ec1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster centroids:\n",
            "[9.    9.125]\n",
            "[0.5   0.925]\n",
            "Result:\n",
            "+-----------+----------+\n",
            "|   features|prediction|\n",
            "+-----------+----------+\n",
            "|  [0.0,0.1]|         1|\n",
            "|  [1.0,1.0]|         1|\n",
            "|  [0.5,0.6]|         1|\n",
            "|  [0.5,2.0]|         1|\n",
            "|  [9.0,8.0]|         0|\n",
            "|  [8.0,9.0]|         0|\n",
            "|  [9.0,9.5]|         0|\n",
            "|[10.0,10.0]|         0|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark ML library - Classification"
      ],
      "metadata": {
        "id": "nzBiKysFstlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "F2KiGl37wXz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "spark = SparkSession.builder.appName(\"IrisLogisticRegression\").getOrCreate()\n",
        "\n",
        "iris_data = spark.read.option('header', 'true').option('inferSchema', 'true').csv('Iris.csv')"
      ],
      "metadata": {
        "id": "_ulVMtU8s9D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "idx_dict = {'Iris-setosa': 0,\n",
        "            'Iris-versicolor': 1,\n",
        "            'Iris-virginica': 2}\n",
        "\n",
        "# User Define Function\n",
        "label_mapping_udf = udf(lambda label: idx_dict.get(label), IntegerType())\n",
        "iris_data = iris_data.withColumn(\"Species\", label_mapping_udf(iris_data[\"Species\"]))\n",
        "iris_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twa3ULuksuzP",
        "outputId": "3c2e36bf-0520-482d-e233-201d47af10d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------------+-------------+------------+-------+\n",
            "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|\n",
            "+---+-------------+------------+-------------+------------+-------+\n",
            "|  1|          5.1|         3.5|          1.4|         0.2|      0|\n",
            "|  2|          4.9|         3.0|          1.4|         0.2|      0|\n",
            "|  3|          4.7|         3.2|          1.3|         0.2|      0|\n",
            "|  4|          4.6|         3.1|          1.5|         0.2|      0|\n",
            "|  5|          5.0|         3.6|          1.4|         0.2|      0|\n",
            "|  6|          5.4|         3.9|          1.7|         0.4|      0|\n",
            "|  7|          4.6|         3.4|          1.4|         0.3|      0|\n",
            "|  8|          5.0|         3.4|          1.5|         0.2|      0|\n",
            "|  9|          4.4|         2.9|          1.4|         0.2|      0|\n",
            "| 10|          4.9|         3.1|          1.5|         0.1|      0|\n",
            "| 11|          5.4|         3.7|          1.5|         0.2|      0|\n",
            "| 12|          4.8|         3.4|          1.6|         0.2|      0|\n",
            "| 13|          4.8|         3.0|          1.4|         0.1|      0|\n",
            "| 14|          4.3|         3.0|          1.1|         0.1|      0|\n",
            "| 15|          5.8|         4.0|          1.2|         0.2|      0|\n",
            "| 16|          5.7|         4.4|          1.5|         0.4|      0|\n",
            "| 17|          5.4|         3.9|          1.3|         0.4|      0|\n",
            "| 18|          5.1|         3.5|          1.4|         0.3|      0|\n",
            "| 19|          5.7|         3.8|          1.7|         0.3|      0|\n",
            "| 20|          5.1|         3.8|          1.5|         0.3|      0|\n",
            "+---+-------------+------------+-------------+------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the features into a vector column and name the column to \"features\"\n",
        "assembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n",
        "                            outputCol=\"features\")\n",
        "\n",
        "# Rename the target column to \"label\"\n",
        "iris_data = assembler.transform(iris_data).select(\"features\", \"Species\").withColumnRenamed(\"Species\", \"label\")"
      ],
      "metadata": {
        "id": "b5laqOWdufZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = iris_data.randomSplit([0.8, 0.2], seed=2023)"
      ],
      "metadata": {
        "id": "sjyqb42OuusD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "logistic_regression = LogisticRegression(featuresCol=\"features\",\n",
        "                                         labelCol=\"label\",\n",
        "                                         predictionCol='prediction',\n",
        "                                         maxIter=100)\n",
        "\n",
        "model = logistic_regression.fit(train_data)"
      ],
      "metadata": {
        "id": "oC_lhCLwuwjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5LpEI8uu2BX",
        "outputId": "53d30a6d-1421-410c-99eb-4b8247246044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel: uid=LogisticRegression_06b914faa6d6, numClasses=3, numFeatures=4"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(test_data)"
      ],
      "metadata": {
        "id": "sw9okWngvcwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1i_7piRwKaZ",
        "outputId": "b026795e-0927-404c-d1f7-9c676a7a9ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+--------------------+--------------------+----------+\n",
            "|         features|label|       rawPrediction|         probability|prediction|\n",
            "+-----------------+-----+--------------------+--------------------+----------+\n",
            "|[4.7,3.2,1.3,0.2]|    0|[9438.32169347803...|       [1.0,0.0,0.0]|       0.0|\n",
            "|[4.9,3.1,1.5,0.1]|    0|[8794.92750919444...|       [1.0,0.0,0.0]|       0.0|\n",
            "|[5.0,3.6,1.4,0.2]|    0|[9604.88573039206...|       [1.0,0.0,0.0]|       0.0|\n",
            "|[5.4,3.9,1.7,0.4]|    0|[8318.84741194077...|       [1.0,0.0,0.0]|       0.0|\n",
            "|[5.5,2.4,3.7,1.0]|    1|[-711.8096549038,...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.5,2.5,4.0,1.3]|    1|[-1965.4704489795...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.6,3.0,4.1,1.3]|    1|[-1128.9673545014...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.6,3.0,4.5,1.5]|    1|[-2440.3856093591...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.7,2.8,4.1,1.3]|    1|[-1826.3269796827...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[5.8,2.7,4.1,1.0]|    1|[-1258.7122062005...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.1,2.8,4.7,1.2]|    1|[-3285.6376804798...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.1,3.0,4.9,1.8]|    2|[-5163.7950144830...|[0.0,2.4006923202...|       2.0|\n",
            "|[6.2,2.2,4.5,1.5]|    1|[-5658.1697700489...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,2.3,4.4,1.3]|    1|[-4791.2041381852...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,2.5,4.9,1.5]|    1|[-5776.7266613810...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,2.8,5.1,1.5]|    2|[-5366.5285956567...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,3.3,4.7,1.6]|    1|[-3870.5243538655...|       [0.0,1.0,0.0]|       1.0|\n",
            "|[6.3,3.4,5.6,2.4]|    2|[-7773.5665309372...|       [0.0,0.0,1.0]|       2.0|\n",
            "|[6.4,2.7,5.3,1.9]|    2|[-7501.3849517314...|       [0.0,0.0,1.0]|       2.0|\n",
            "|[6.4,2.8,5.6,2.2]|    2|[-8755.0457458071...|       [0.0,0.0,1.0]|       2.0|\n",
            "+-----------------+-----+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7kdWC0awM0F",
        "outputId": "56c0cca6-4687-497d-e596-d3df3bc1d7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fypJDEgEwRrF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}